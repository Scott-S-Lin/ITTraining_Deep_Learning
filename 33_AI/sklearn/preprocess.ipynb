{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入Iris資料集\n",
    "\n",
    "Iris Flowers 資料集\n",
    "\n",
    "我們在這個項目中使用 Iris Data Set，這個資料集中的每個樣本有4個特徵，1個類別。該資料集1中的樣本類別數為3類，每類樣本數目為50個，總共150個樣本。\n",
    "\n",
    "屬性資訊：\n",
    "\n",
    "    花萼長度 sepal length(cm)\n",
    "    花萼寬度 sepal width(cm)\n",
    "    花瓣長度 petal length(cm)\n",
    "    花瓣寬度 petal width(cm)\n",
    "    類別：\n",
    "        Iris Setosa\n",
    "        Iris Versicolour\n",
    "        Iris Virginica\n",
    "\n",
    "樣本特徵資料是數值型的，而且單位都相同（釐米）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, :2] # we only take the first two features. We could\n",
    "print(X.shape)\n",
    "Y = iris.target\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下是組成 pandas DataFrame (也可以不用這種做)\n",
    "x = pd.DataFrame(iris['data'], columns=iris['feature_names'])\n",
    "print(iris['data'].size)\n",
    "print(\"target_names: \"+str(iris['target_names']))\n",
    "y = pd.DataFrame(iris['target'], columns=['target'])\n",
    "iris_data = pd.concat([x,y], axis=1)\n",
    "iris_data = iris_data[['sepal length (cm)','petal length (cm)','target']]\n",
    "iris_data = iris_data[iris_data['target'].isin([0,1])]\n",
    "iris_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris_data[['sepal length (cm)','petal length (cm)']], iris_data[['target']], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "    to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler().fit(X_train)  #Compute the statistics to be used for later scaling.\n",
    "print(sc.mean_)  #mean\n",
    "print(sc.scale_) #standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform: (x-u)/std.\n",
    "X_train_std = sc.transform(X_train)\n",
    "print(X_train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaler instance can then be used on new data to transform it the same way it did on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_std = sc.transform(X_test)\n",
    "print(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also use fit_transform method (i.e., fit and then transform)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                      \n",
    "X_train_std = sc.fit_transform(X_train)  \n",
    "X_test_std = sc.fit_transform(X_test)\n",
    "print(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normaliaztion\n",
    "    Transforms features by scaling each feature to a given range.\n",
    "    The transformation is given by:\n",
    "\n",
    "    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "    X_scaled = X_std * (max - min) + min\n",
    "    where min, max = feature_range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.normal(50, 6, 100)  # np.random.normal(mu,sigma,size))\n",
    "y1 = np.random.normal(5, 0.5, 100)\n",
    "\n",
    "x2 = np.random.normal(30,6,100)\n",
    "y2 = np.random.normal(4,0.5,100)\n",
    "plt.scatter(x1,y1,c='b',marker='s',s=20,alpha=0.8)\n",
    "plt.scatter(x2,y2,c='r', marker='^', s=20, alpha=0.8)\n",
    "\n",
    "print(np.sum(x1)/len(x1))\n",
    "print(np.sum(x2)/len(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = np.concatenate((x1,x2))\n",
    "y_val = np.concatenate((y1,y2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "x_val=x_val.reshape(-1, 1)\n",
    "scaler = MinMaxScaler().fit(x_val)  # default range 0~1\n",
    "print(scaler.data_max_)\n",
    "print(scaler.transform(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sometime you may use this\n",
    "x_diff = max(x_val)-min(x_val)\n",
    "y_diff = max(y_val)-min(y_val)\n",
    "x_normalized = [x/(x_diff) for x in x_val]\n",
    "y_normalized = [y/(y_diff) for y in y_val]\n",
    "xy_normalized = zip(x_normalized,y_normalized)\n",
    "print(x_normalized)\n",
    "#創建label\n",
    "labels = [1]*200+[2]*200\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sigmoid funcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "z = np.arange(-7, 7, 0.1)\n",
    "phi_z = sigmoid(z)\n",
    "\n",
    "plt.plot(z, phi_z)\n",
    "plt.axvline(0.0, color='k')\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('$\\phi (z)$')\n",
    "\n",
    "# y axis ticks and gridline\n",
    "plt.yticks([0.0, 0.5, 1.0])\n",
    "ax = plt.gca()\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./figures/sigmoid.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
